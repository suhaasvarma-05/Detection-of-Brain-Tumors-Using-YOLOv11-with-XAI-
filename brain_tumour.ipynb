{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suhaasvarma-05/Detection-of-Brain-Tumors-Using-YOLOv11-with-XAI-/blob/main/brain_tumour.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdcDLGokawUo",
        "outputId": "f4c91075-b049-4106-fbf5-b801419f3a00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dkh3jse3bGKF",
        "outputId": "053c6435-81a0-4128-e0c2-124625a9c284"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.2.11-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow) (2025.10.5)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.4.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from roboflow) (11.3.0)\n",
            "Collecting pi-heif<2 (from roboflow)\n",
            "  Downloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting pillow-avif-plugin<2 (from roboflow)\n",
            "  Downloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.32.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.5.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (6.0.3)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (1.3.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (4.60.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (3.2.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow) (3.4.4)\n",
            "Downloading roboflow-1.2.11-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: pillow-avif-plugin, filetype, pi-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.12.0.88\n",
            "    Uninstalling opencv-python-headless-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.11\n",
            "    Uninstalling idna-3.11:\n",
            "      Successfully uninstalled idna-3.11\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pi-heif-1.1.1 pillow-avif-plugin-1.5.2 roboflow-1.2.11\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7L2fPOTebJ01"
      },
      "outputs": [],
      "source": [
        "from roboflow import Roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WlcIVIW5bUEC",
        "outputId": "198a1f31-55c3-40ed-f6ed-dcbb8e20f2cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading Dataset Version Zip in Brain-Tumor-Detection-1 to yolov11:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105499/105499 [00:01<00:00, 54521.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Brain-Tumor-Detection-1 in yolov11:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9128/9128 [00:01<00:00, 7648.91it/s]\n"
          ]
        }
      ],
      "source": [
        "rf=Roboflow(api_key=\"CE4lpbKi4sP2Hk9PrrEB\")\n",
        "project=rf.workspace(\"braindata\").project(\"brain-tumor-detection-aaerl\")\n",
        "version=project.version(1)\n",
        "dataset=version.download(\"yolov11\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "k8SrBc6xbWii",
        "outputId": "2894ccb4-d1cb-4495-cb04-ad55a2a306d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.221-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.17-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.10.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.3.221-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.17-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.221 ultralytics-thop-2.0.17\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics opencv-python pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1P7giiLZbnTu",
        "outputId": "114ef6af-3dc4-4663-abfe-e193d88966f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.4MB 70.1MB/s 0.1s\n",
            "Ultralytics 8.3.221 ğŸš€ Python-3.12.12 torch-2.8.0+cu126 \n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Invalid CUDA 'device=0' requested. Use 'device=cpu' or pass valid CUDA device(s) if available, i.e. 'device=0' or 'device=0,1,2,3' for Multi-GPU.\n\ntorch.cuda.is_available(): False\ntorch.cuda.device_count(): 0\nos.environ['CUDA_VISIBLE_DEVICES']: None\nSee https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no CUDA devices are seen by torch.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-348958961.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m train_results = model.train(\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/Brain-Tumor-Detection-1/data.yaml\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# path to dataset YAML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"resume\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_smart_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trainer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resume\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# manually set model only if not resuming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/models/yolo/detect/train.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg, overrides, _callbacks)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0m_callbacks\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mexecuted\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \"\"\"\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg, overrides, _callbacks)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cfg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;31m# Update \"-1\" devices so post-training val does not repeat search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CUDA_VISIBLE_DEVICES\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/utils/torch_utils.py\u001b[0m in \u001b[0;36mselect_device\u001b[0;34m(device, newline, verbose)\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             )\n\u001b[0;32m--> 200\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    201\u001b[0m                 \u001b[0;34mf\"Invalid CUDA 'device={device}' requested.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0;34mf\" Use 'device=cpu' or pass valid CUDA device(s) if available,\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid CUDA 'device=0' requested. Use 'device=cpu' or pass valid CUDA device(s) if available, i.e. 'device=0' or 'device=0,1,2,3' for Multi-GPU.\n\ntorch.cuda.is_available(): False\ntorch.cuda.device_count(): 0\nos.environ['CUDA_VISIBLE_DEVICES']: None\nSee https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no CUDA devices are seen by torch.\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "\n",
        "\n",
        "model = YOLO(\"yolo11n.pt\")\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "\n",
        "train_results = model.train(\n",
        "\n",
        "    data=\"/content/Brain-Tumor-Detection-1/data.yaml\",  # path to dataset YAML\n",
        "\n",
        "    epochs=50,  # number of training epochs\n",
        "\n",
        "    imgsz=640,  # training image size\n",
        "\n",
        "    device=0,  # device to run on, i.e. device=0 or device=0,1,2,3 or device=cpu\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xET8l-VAm5Aw"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "\n",
        "\n",
        "# Load a model\n",
        "\n",
        "model = YOLO(\"/content/runs/detect/train/weights/best.pt\")\n",
        "\n",
        "\n",
        "\n",
        "# Perform object detection on an image\n",
        "\n",
        "results = model(\"/content/Brain-Tumor-Detection-1/test/images/Te-me_0130_jpg.rf.0f612b364bfe9f63fcfdedfcb30cf90c.jpg\", save=True)\n",
        "\n",
        "results[0].show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "G7WNbF2Jo-GX"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "from ultralytics import SAM\n",
        "\n",
        "\n",
        "\n",
        "# Load the YOLO model\n",
        "\n",
        "yolo_model = YOLO(\"/content/runs/detect/train/weights/best.pt\")  # pretrained YOLO model\n",
        "\n",
        "\n",
        "\n",
        "# Run batched inference on a list of images\n",
        "\n",
        "results = yolo_model(\"/content/Brain-Tumor-Detection-1/test/images/Te-me_0130_jpg.rf.0f612b364bfe9f63fcfdedfcb30cf90c.jpg\")\n",
        "sam_model = SAM(\"sam2_b.pt\")\n",
        "\n",
        "\n",
        "\n",
        "for result in results:\n",
        "\n",
        "     class_ids = result.boxes.cls.int().tolist()  # noqa\n",
        "\n",
        "     if len(class_ids):\n",
        "\n",
        "         boxes = result.boxes.xyxy  # Boxes object for bbox outputs\n",
        "\n",
        "         sam_results = sam_model(result.orig_img, bboxes=boxes, verbose=False, save=True, device=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6484c7fe"
      },
      "outputs": [],
      "source": [
        "# Extract and display accuracy metrics\n",
        "# mAP50 is a common metric for object detection accuracy\n",
        "print(\"Accuracy (mAP50):\", train_results.results_dict['metrics/mAP50(B)'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ItSlp-lLpr6k"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "display(Image('/content/runs/segment/predict/image0.jpg'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IYzcVBaepxEf"
      },
      "outputs": [],
      "source": [
        "# Show Confusion Matrix\n",
        "from IPython.display import Image, display\n",
        "display(Image('/content/runs/detect/train/confusion_matrix_normalized.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nuRvN8PvrOks"
      },
      "outputs": [],
      "source": [
        "# Show Confusion Matrix\n",
        "from IPython.display import Image, display\n",
        "display(Image('/content/runs/detect/predict/Te-me_0130_jpg.rf.0f612b364bfe9f63fcfdedfcb30cf90c.jpg'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "L6pjhEl2sHaB"
      },
      "outputs": [],
      "source": [
        "# Install required packages (no pytorch-grad-cam needed)\n",
        "!pip install ultralytics opencv-python pandas matplotlib seaborn\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ============================================\n",
        "# 1. Improved YOLO Grad-CAM Implementation\n",
        "# ============================================\n",
        "\n",
        "class YOLOGradCAM:\n",
        "    \"\"\"Grad-CAM implementation for YOLO models\"\"\"\n",
        "\n",
        "    def __init__(self, model_path):\n",
        "        self.model = YOLO(model_path)\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def generate_gradcam(self, image_path):\n",
        "        \"\"\"Generate Grad-CAM heatmap for an image\"\"\"\n",
        "\n",
        "        # Read and preprocess image\n",
        "        img = cv2.imread(image_path)\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        original_shape = img_rgb.shape[:2]\n",
        "\n",
        "        # Get prediction first\n",
        "        results = self.model(image_path, verbose=False)\n",
        "\n",
        "        # Prepare image for processing\n",
        "        img_resized = cv2.resize(img_rgb, (640, 640))\n",
        "        img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
        "        img_tensor = img_tensor.to(self.device)\n",
        "        img_tensor.requires_grad = True\n",
        "\n",
        "        # Storage for features and gradients\n",
        "        features = []\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            features.append(output.clone().detach().requires_grad_(True))\n",
        "\n",
        "        # Register hook on a middle layer\n",
        "        target_layer = self.model.model.model[-4]  # Use layer before head\n",
        "        handle = target_layer.register_forward_hook(forward_hook)\n",
        "\n",
        "        try:\n",
        "            # Forward pass\n",
        "            with torch.set_grad_enabled(True):\n",
        "                output = self.model.model(img_tensor)\n",
        "\n",
        "            # Get the feature map\n",
        "            if len(features) > 0:\n",
        "                feature_map = features[0]\n",
        "\n",
        "                # Create a simple target score (sum of all outputs)\n",
        "                if isinstance(output, (list, tuple)):\n",
        "                    # For YOLO v8/v11 format\n",
        "                    target = output[0].sum()\n",
        "                else:\n",
        "                    target = output.sum()\n",
        "\n",
        "                # Compute gradients\n",
        "                target.backward(retain_graph=True)\n",
        "\n",
        "                # Get gradients\n",
        "                if feature_map.grad is not None:\n",
        "                    gradients = feature_map.grad\n",
        "\n",
        "                    # Global average pooling on gradients\n",
        "                    weights = torch.mean(gradients, dim=[2, 3], keepdim=True)\n",
        "\n",
        "                    # Weighted combination of feature maps\n",
        "                    cam = torch.sum(weights * feature_map, dim=1, keepdim=True)\n",
        "                    cam = F.relu(cam)\n",
        "\n",
        "                    # Normalize\n",
        "                    cam = cam.squeeze().cpu().detach().numpy()\n",
        "                    cam = cam - cam.min()\n",
        "                    if cam.max() > 0:\n",
        "                        cam = cam / cam.max()\n",
        "\n",
        "                    # Resize to original image size\n",
        "                    cam = cv2.resize(cam, (original_shape[1], original_shape[0]))\n",
        "                else:\n",
        "                    # Fallback: use feature map activations\n",
        "                    cam = torch.mean(feature_map, dim=1).squeeze().cpu().detach().numpy()\n",
        "                    cam = np.maximum(cam, 0)\n",
        "                    if cam.max() > 0:\n",
        "                        cam = cam / cam.max()\n",
        "                    cam = cv2.resize(cam, (original_shape[1], original_shape[0]))\n",
        "            else:\n",
        "                # Create empty heatmap\n",
        "                cam = np.zeros(original_shape)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Grad-CAM error: {e}\")\n",
        "            # Fallback: create heatmap based on detection boxes\n",
        "            cam = np.zeros(original_shape)\n",
        "            if len(results[0].boxes) > 0:\n",
        "                for box, conf in zip(results[0].boxes.xyxy, results[0].boxes.conf):\n",
        "                    x1, y1, x2, y2 = map(int, box.tolist())\n",
        "                    cam[y1:y2, x1:x2] = conf.item()\n",
        "                if cam.max() > 0:\n",
        "                    cam = cam / cam.max()\n",
        "\n",
        "        finally:\n",
        "            handle.remove()\n",
        "\n",
        "        return cam, img_rgb, results\n",
        "\n",
        "# ============================================\n",
        "# 2. Saliency Map Implementation\n",
        "# ============================================\n",
        "\n",
        "def generate_saliency_map(model, image_path):\n",
        "    \"\"\"Generate saliency map showing pixel importance\"\"\"\n",
        "\n",
        "    img = cv2.imread(image_path)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    original_shape = img_rgb.shape[:2]\n",
        "\n",
        "    try:\n",
        "        img_resized = cv2.resize(img_rgb, (640, 640))\n",
        "        img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
        "        img_tensor = img_tensor.to(model.device)\n",
        "        img_tensor.requires_grad = True\n",
        "\n",
        "        # Forward pass\n",
        "        output = model.model(img_tensor)\n",
        "\n",
        "        # Compute score\n",
        "        if isinstance(output, (list, tuple)):\n",
        "            score = output[0].sum()\n",
        "        else:\n",
        "            score = output.sum()\n",
        "\n",
        "        # Backward pass\n",
        "        score.backward()\n",
        "\n",
        "        # Get gradients\n",
        "        saliency = img_tensor.grad.data.abs()\n",
        "        saliency = saliency.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
        "        saliency = np.max(saliency, axis=2)\n",
        "\n",
        "        # Normalize\n",
        "        if saliency.max() > 0:\n",
        "            saliency = saliency / saliency.max()\n",
        "\n",
        "        # Resize to original\n",
        "        saliency = cv2.resize(saliency, (original_shape[1], original_shape[0]))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Saliency map error: {e}\")\n",
        "        saliency = np.zeros(original_shape)\n",
        "\n",
        "    return saliency, img_rgb\n",
        "\n",
        "# ============================================\n",
        "# 3. Attention Visualization\n",
        "# ============================================\n",
        "\n",
        "def visualize_attention_regions(results, image_path):\n",
        "    \"\"\"Visualize detected regions with confidence scores\"\"\"\n",
        "\n",
        "    img = cv2.imread(image_path)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Create attention map\n",
        "    attention_map = np.zeros((img.shape[0], img.shape[1]), dtype=np.float32)\n",
        "\n",
        "    if len(results[0].boxes) > 0:\n",
        "        for box, conf in zip(results[0].boxes.xyxy, results[0].boxes.conf):\n",
        "            x1, y1, x2, y2 = map(int, box.tolist())\n",
        "            # Ensure coordinates are within image bounds\n",
        "            x1, y1 = max(0, x1), max(0, y1)\n",
        "            x2, y2 = min(img.shape[1], x2), min(img.shape[0], y2)\n",
        "\n",
        "            # Add weighted attention\n",
        "            attention_map[y1:y2, x1:x2] += conf.item()\n",
        "\n",
        "    # Normalize\n",
        "    if attention_map.max() > 0:\n",
        "        attention_map = attention_map / attention_map.max()\n",
        "\n",
        "    return attention_map, img_rgb\n",
        "\n",
        "# ============================================\n",
        "# 4. Feature Visualization\n",
        "# ============================================\n",
        "\n",
        "def extract_and_visualize_features(model, image_path):\n",
        "    \"\"\"Extract and visualize intermediate feature maps\"\"\"\n",
        "\n",
        "    img = cv2.imread(image_path)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img_resized = cv2.resize(img_rgb, (640, 640))\n",
        "    img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
        "    img_tensor = img_tensor.to(model.device)\n",
        "\n",
        "    features = []\n",
        "\n",
        "    def hook(module, input, output):\n",
        "        features.append(output.clone().detach())\n",
        "\n",
        "    # Hook into multiple layers\n",
        "    layers_to_hook = [-6, -4, -2]\n",
        "    handles = []\n",
        "\n",
        "    for layer_idx in layers_to_hook:\n",
        "        try:\n",
        "            target_layer = model.model.model[layer_idx]\n",
        "            handle = target_layer.register_forward_hook(hook)\n",
        "            handles.append(handle)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        _ = model.model(img_tensor)\n",
        "\n",
        "    # Remove hooks\n",
        "    for handle in handles:\n",
        "        handle.remove()\n",
        "\n",
        "    return features, img_rgb\n",
        "\n",
        "# ============================================\n",
        "# 5. Comprehensive Visualization\n",
        "# ============================================\n",
        "\n",
        "def comprehensive_xai_visualization(model_path, image_path, output_path='xai_analysis.png'):\n",
        "    \"\"\"Create comprehensive XAI visualization\"\"\"\n",
        "\n",
        "    print(\"Loading model and generating XAI visualizations...\")\n",
        "\n",
        "    # Load model\n",
        "    model = YOLO(model_path)\n",
        "\n",
        "    # Original prediction\n",
        "    print(\"Running inference...\")\n",
        "    results = model(image_path, verbose=False)\n",
        "\n",
        "    # Initialize Grad-CAM\n",
        "    print(\"Generating Grad-CAM...\")\n",
        "    gradcam = YOLOGradCAM(model_path)\n",
        "    heatmap, img_rgb, _ = gradcam.generate_gradcam(image_path)\n",
        "\n",
        "    print(\"Generating Saliency Map...\")\n",
        "    saliency, _ = generate_saliency_map(model, image_path)\n",
        "\n",
        "    print(\"Generating Attention Map...\")\n",
        "    attention, _ = visualize_attention_regions(results, image_path)\n",
        "\n",
        "    print(\"Extracting Features...\")\n",
        "    features, _ = extract_and_visualize_features(model, image_path)\n",
        "\n",
        "    # Create visualization\n",
        "    print(\"Creating visualization...\")\n",
        "    fig = plt.figure(figsize=(20, 14))\n",
        "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "    # Row 1: Original, Prediction, Grad-CAM\n",
        "    ax1 = fig.add_subplot(gs[0, 0])\n",
        "    ax1.imshow(img_rgb)\n",
        "    ax1.set_title('Original Image', fontsize=16, fontweight='bold', pad=10)\n",
        "    ax1.axis('off')\n",
        "\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    pred_img = results[0].plot()\n",
        "    ax2.imshow(cv2.cvtColor(pred_img, cv2.COLOR_BGR2RGB))\n",
        "    ax2.set_title('Model Prediction', fontsize=16, fontweight='bold', pad=10)\n",
        "    ax2.axis('off')\n",
        "\n",
        "    ax3 = fig.add_subplot(gs[0, 2])\n",
        "    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
        "    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
        "    superimposed = cv2.addWeighted(img_rgb, 0.5, heatmap_colored, 0.5, 0)\n",
        "    ax3.imshow(superimposed)\n",
        "    ax3.set_title('Grad-CAM Overlay', fontsize=16, fontweight='bold', pad=10)\n",
        "    ax3.axis('off')\n",
        "\n",
        "    # Row 2: Saliency, Attention, Pure Heatmap\n",
        "    ax4 = fig.add_subplot(gs[1, 0])\n",
        "    im4 = ax4.imshow(saliency, cmap='hot')\n",
        "    ax4.set_title('Saliency Map (Pixel Importance)', fontsize=16, fontweight='bold', pad=10)\n",
        "    ax4.axis('off')\n",
        "    plt.colorbar(im4, ax=ax4, fraction=0.046)\n",
        "\n",
        "    ax5 = fig.add_subplot(gs[1, 1])\n",
        "    im5 = ax5.imshow(attention, cmap='viridis')\n",
        "    ax5.set_title('Attention Regions', fontsize=16, fontweight='bold', pad=10)\n",
        "    ax5.axis('off')\n",
        "    plt.colorbar(im5, ax=ax5, fraction=0.046)\n",
        "\n",
        "    ax6 = fig.add_subplot(gs[1, 2])\n",
        "    im6 = ax6.imshow(heatmap, cmap='jet')\n",
        "    ax6.set_title('Activation Heatmap', fontsize=16, fontweight='bold', pad=10)\n",
        "    ax6.axis('off')\n",
        "    plt.colorbar(im6, ax=ax6, fraction=0.046)\n",
        "\n",
        "    # Row 3: Feature maps from different layers\n",
        "    if features and len(features) > 0:\n",
        "        for idx, (feature, ax_idx) in enumerate(zip(features[:3], [7, 8, 9])):\n",
        "            ax = fig.add_subplot(gs[2, ax_idx-7])\n",
        "\n",
        "            # Average across channels for visualization\n",
        "            feat_vis = torch.mean(feature[0], dim=0).cpu().numpy()\n",
        "            feat_vis = (feat_vis - feat_vis.min()) / (feat_vis.max() - feat_vis.min() + 1e-8)\n",
        "\n",
        "            im = ax.imshow(feat_vis, cmap='viridis')\n",
        "            ax.set_title(f'Feature Map - Layer {idx+1}', fontsize=14, fontweight='bold', pad=10)\n",
        "            ax.axis('off')\n",
        "            plt.colorbar(im, ax=ax, fraction=0.046)\n",
        "\n",
        "    # Add detection details\n",
        "    detection_text = \"ğŸ” Detection Results:\\n\" + \"=\"*50 + \"\\n\"\n",
        "    if len(results[0].boxes) > 0:\n",
        "        for i, (box, conf, cls) in enumerate(zip(results[0].boxes.xyxy,\n",
        "                                                  results[0].boxes.conf,\n",
        "                                                  results[0].boxes.cls)):\n",
        "            class_name = results[0].names[int(cls.item())]\n",
        "            detection_text += f\"  {i+1}. {class_name.upper()}: {conf.item()*100:.2f}% confidence\\n\"\n",
        "    else:\n",
        "        detection_text += \"  No tumors detected\\n\"\n",
        "\n",
        "    detection_text += \"=\"*50 + \"\\n\"\n",
        "    detection_text += \"ğŸ“Š XAI Methods:\\n\"\n",
        "    detection_text += \"  â€¢ Grad-CAM: Neural activation regions\\n\"\n",
        "    detection_text += \"  â€¢ Saliency: Pixel-level importance\\n\"\n",
        "    detection_text += \"  â€¢ Attention: Confidence-weighted focus\\n\"\n",
        "\n",
        "    plt.figtext(0.5, 0.01, detection_text, ha='center', fontsize=11,\n",
        "               family='monospace',\n",
        "               bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
        "\n",
        "    plt.suptitle('Brain Tumor Detection - Explainable AI Analysis',\n",
        "                fontsize=20, fontweight='bold', y=0.98)\n",
        "\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "    print(f\"âœ… XAI visualization saved to: {output_path}\")\n",
        "    plt.show()\n",
        "\n",
        "# ============================================\n",
        "# 6. Generate XAI Report\n",
        "# ============================================\n",
        "\n",
        "def generate_xai_report(model_path, image_path):\n",
        "    \"\"\"Generate detailed XAI report\"\"\"\n",
        "\n",
        "    model = YOLO(model_path)\n",
        "    results = model(image_path, verbose=False)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ğŸ§  BRAIN TUMOR DETECTION - EXPLAINABLE AI ANALYSIS REPORT\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nğŸ“ Image: {image_path.split('/')[-1]}\")\n",
        "    print(f\"ğŸ¤– Model: {model_path.split('/')[-1]}\")\n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "\n",
        "    if len(results[0].boxes) > 0:\n",
        "        print(\"\\nğŸ¯ DETECTIONS:\")\n",
        "        total_confidence = 0\n",
        "        for i, (box, conf, cls) in enumerate(zip(results[0].boxes.xyxy,\n",
        "                                                  results[0].boxes.conf,\n",
        "                                                  results[0].boxes.cls)):\n",
        "            class_name = results[0].names[int(cls.item())]\n",
        "            x1, y1, x2, y2 = box.tolist()\n",
        "            area = (x2-x1)*(y2-y1)\n",
        "            total_confidence += conf.item()\n",
        "\n",
        "            print(f\"\\n  Detection #{i+1}:\")\n",
        "            print(f\"    â”œâ”€ Class: {class_name.upper()}\")\n",
        "            print(f\"    â”œâ”€ Confidence: {conf.item():.4f} ({conf.item()*100:.2f}%)\")\n",
        "            print(f\"    â”œâ”€ Location: ({x1:.0f}, {y1:.0f}) â†’ ({x2:.0f}, {y2:.0f})\")\n",
        "            print(f\"    â””â”€ Area: {area:.0f} pixelsÂ² ({(area/(results[0].orig_shape[0]*results[0].orig_shape[1]))*100:.2f}% of image)\")\n",
        "\n",
        "        avg_conf = total_confidence / len(results[0].boxes)\n",
        "        print(f\"\\n  ğŸ“Š Average Confidence: {avg_conf*100:.2f}%\")\n",
        "    else:\n",
        "        print(\"\\nâŒ No tumors detected in the image.\")\n",
        "\n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "    print(\"\\nğŸ’¡ EXPLAINABILITY METHODS:\")\n",
        "    print(\"  1. Grad-CAM (Gradient-weighted Class Activation Mapping)\")\n",
        "    print(\"     â†’ Highlights which regions activated the neural network\")\n",
        "    print(\"     â†’ Shows spatial importance for tumor detection\")\n",
        "    print(\"\\n  2. Saliency Map\")\n",
        "    print(\"     â†’ Displays pixel-level importance using gradients\")\n",
        "    print(\"     â†’ Indicates which pixels most influence the prediction\")\n",
        "    print(\"\\n  3. Attention Visualization\")\n",
        "    print(\"     â†’ Shows model's focus areas weighted by confidence\")\n",
        "    print(\"     â†’ Combines detection locations with certainty scores\")\n",
        "    print(\"\\n  4. Feature Map Analysis\")\n",
        "    print(\"     â†’ Reveals intermediate layer representations\")\n",
        "    print(\"     â†’ Shows patterns and features learned by the network\")\n",
        "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "# ============================================\n",
        "# 7. Main Execution\n",
        "# ============================================\n",
        "\n",
        "# Use your trained model\n",
        "MODEL_PATH = \"/content/runs/detect/train/weights/best.pt\"\n",
        "IMAGE_PATH = \"/content/Brain-Tumor-Detection-1/test/images/Te-me_0159_jpg.rf.7afad29587df72dcfba2e189a75656fa.jpg\"\n",
        "\n",
        "# Generate comprehensive XAI visualization\n",
        "comprehensive_xai_visualization(MODEL_PATH, IMAGE_PATH, 'brain_tumor_xai_analysis.png')\n",
        "\n",
        "# Display the result\n",
        "from IPython.display import Image, display\n",
        "display(Image('brain_tumor_xai_analysis.png'))\n",
        "\n",
        "# Generate detailed report\n",
        "generate_xai_report(MODEL_PATH, IMAGE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TSDzI-7cvtQX"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install ultralytics opencv-python pandas matplotlib seaborn numpy scipy\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from ultralytics import YOLO\n",
        "from scipy.spatial import ConvexHull\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import json\n",
        "\n",
        "# ============================================\n",
        "# 1. Volumetric Analysis Core Functions\n",
        "# ============================================\n",
        "\n",
        "class BrainTumorVolumetricAnalyzer:\n",
        "    \"\"\"Comprehensive volumetric analysis for brain tumor detection\"\"\"\n",
        "\n",
        "    def __init__(self, model_path, pixel_spacing=1.0, slice_thickness=1.0):\n",
        "        \"\"\"\n",
        "        Initialize analyzer\n",
        "\n",
        "        Args:\n",
        "            model_path: Path to YOLO model\n",
        "            pixel_spacing: Physical size of pixel in mm (default: 1.0 mm)\n",
        "            slice_thickness: Thickness between slices in mm (default: 1.0 mm)\n",
        "        \"\"\"\n",
        "        self.model = YOLO(model_path)\n",
        "        self.pixel_spacing = pixel_spacing  # mm per pixel\n",
        "        self.slice_thickness = slice_thickness  # mm between slices\n",
        "\n",
        "    def analyze_single_image(self, image_path):\n",
        "        \"\"\"Perform volumetric analysis on a single image\"\"\"\n",
        "\n",
        "        # Run detection\n",
        "        results = self.model(image_path, verbose=False)\n",
        "\n",
        "        # Read image\n",
        "        img = cv2.imread(image_path)\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        analyses = []\n",
        "\n",
        "        if len(results[0].boxes) > 0:\n",
        "            for i, (box, conf, cls) in enumerate(zip(results[0].boxes.xyxy,\n",
        "                                                      results[0].boxes.conf,\n",
        "                                                      results[0].boxes.cls)):\n",
        "\n",
        "                x1, y1, x2, y2 = map(int, box.tolist())\n",
        "                class_name = results[0].names[int(cls.item())]\n",
        "\n",
        "                # Extract tumor region\n",
        "                tumor_region = img_rgb[y1:y2, x1:x2]\n",
        "\n",
        "                # Perform measurements\n",
        "                analysis = self._analyze_tumor_region(\n",
        "                    tumor_region,\n",
        "                    (x1, y1, x2, y2),\n",
        "                    class_name,\n",
        "                    conf.item(),\n",
        "                    img_rgb.shape\n",
        "                )\n",
        "\n",
        "                analyses.append(analysis)\n",
        "\n",
        "        return analyses, results, img_rgb\n",
        "\n",
        "    def _analyze_tumor_region(self, region, bbox, class_name, confidence, img_shape):\n",
        "        \"\"\"Analyze a single tumor region\"\"\"\n",
        "\n",
        "        x1, y1, x2, y2 = bbox\n",
        "\n",
        "        # 2D measurements\n",
        "        width_pixels = x2 - x1\n",
        "        height_pixels = y2 - y1\n",
        "        area_pixels = width_pixels * height_pixels\n",
        "\n",
        "        # Convert to physical measurements (mm)\n",
        "        width_mm = width_pixels * self.pixel_spacing\n",
        "        height_mm = height_pixels * self.pixel_spacing\n",
        "        area_mm2 = area_pixels * (self.pixel_spacing ** 2)\n",
        "\n",
        "        # Estimate 3D volume using different methods\n",
        "\n",
        "        # Method 1: Ellipsoid approximation (most common for tumors)\n",
        "        # V = (4/3) * Ï€ * a * b * c, where c is estimated from average of a and b\n",
        "        a = width_mm / 2\n",
        "        b = height_mm / 2\n",
        "        c = (a + b) / 2  # Estimate depth\n",
        "        volume_ellipsoid = (4/3) * np.pi * a * b * c\n",
        "\n",
        "        # Method 2: Rectangular prism (conservative estimate)\n",
        "        depth_estimate = np.sqrt(width_mm * height_mm)  # Geometric mean\n",
        "        volume_prism = width_mm * height_mm * depth_estimate\n",
        "\n",
        "        # Method 3: Sphere approximation (for round tumors)\n",
        "        diameter = (width_mm + height_mm) / 2\n",
        "        radius = diameter / 2\n",
        "        volume_sphere = (4/3) * np.pi * (radius ** 3)\n",
        "\n",
        "        # Average volume (most reliable estimate)\n",
        "        volume_avg = (volume_ellipsoid + volume_prism + volume_sphere) / 3\n",
        "\n",
        "        # Calculate tumor characteristics\n",
        "        aspect_ratio = width_mm / height_mm if height_mm > 0 else 0\n",
        "        circularity = self._calculate_circularity(region)\n",
        "\n",
        "        # Relative position in brain\n",
        "        center_x = (x1 + x2) / 2\n",
        "        center_y = (y1 + y2) / 2\n",
        "        relative_pos = {\n",
        "            'x_ratio': center_x / img_shape[1],\n",
        "            'y_ratio': center_y / img_shape[0],\n",
        "            'quadrant': self._determine_quadrant(center_x, center_y, img_shape)\n",
        "        }\n",
        "\n",
        "        # Texture analysis\n",
        "        texture = self._analyze_texture(region)\n",
        "\n",
        "        return {\n",
        "            'class': class_name,\n",
        "            'confidence': confidence,\n",
        "            'bbox': bbox,\n",
        "            # 2D measurements\n",
        "            'width_mm': width_mm,\n",
        "            'height_mm': height_mm,\n",
        "            'area_mm2': area_mm2,\n",
        "            'diameter_avg_mm': (width_mm + height_mm) / 2,\n",
        "            # 3D volume estimates\n",
        "            'volume_ellipsoid_mm3': volume_ellipsoid,\n",
        "            'volume_prism_mm3': volume_prism,\n",
        "            'volume_sphere_mm3': volume_sphere,\n",
        "            'volume_average_mm3': volume_avg,\n",
        "            'volume_ml': volume_avg / 1000,  # Convert to milliliters\n",
        "            # Shape characteristics\n",
        "            'aspect_ratio': aspect_ratio,\n",
        "            'circularity': circularity,\n",
        "            'shape_regularity': self._classify_shape(aspect_ratio, circularity),\n",
        "            # Position\n",
        "            'position': relative_pos,\n",
        "            # Texture\n",
        "            'texture': texture\n",
        "        }\n",
        "\n",
        "    def _calculate_circularity(self, region):\n",
        "        \"\"\"Calculate circularity of tumor region (1.0 = perfect circle)\"\"\"\n",
        "\n",
        "        gray = cv2.cvtColor(region, cv2.COLOR_RGB2GRAY)\n",
        "        _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        if contours:\n",
        "            largest_contour = max(contours, key=cv2.contourArea)\n",
        "            area = cv2.contourArea(largest_contour)\n",
        "            perimeter = cv2.arcLength(largest_contour, True)\n",
        "\n",
        "            if perimeter > 0:\n",
        "                circularity = 4 * np.pi * area / (perimeter ** 2)\n",
        "                return min(circularity, 1.0)\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "    def _classify_shape(self, aspect_ratio, circularity):\n",
        "        \"\"\"Classify tumor shape based on measurements\"\"\"\n",
        "\n",
        "        if circularity > 0.8:\n",
        "            return \"Round/Spherical\"\n",
        "        elif 0.8 <= aspect_ratio <= 1.2:\n",
        "            return \"Oval/Elliptical\"\n",
        "        elif aspect_ratio > 1.5 or aspect_ratio < 0.67:\n",
        "            return \"Elongated/Irregular\"\n",
        "        else:\n",
        "            return \"Moderately Irregular\"\n",
        "\n",
        "    def _determine_quadrant(self, x, y, shape):\n",
        "        \"\"\"Determine which quadrant of the brain the tumor is in\"\"\"\n",
        "\n",
        "        mid_x = shape[1] / 2\n",
        "        mid_y = shape[0] / 2\n",
        "\n",
        "        if x < mid_x and y < mid_y:\n",
        "            return \"Upper Left\"\n",
        "        elif x >= mid_x and y < mid_y:\n",
        "            return \"Upper Right\"\n",
        "        elif x < mid_x and y >= mid_y:\n",
        "            return \"Lower Left\"\n",
        "        else:\n",
        "            return \"Lower Right\"\n",
        "\n",
        "    def _analyze_texture(self, region):\n",
        "        \"\"\"Analyze texture characteristics of tumor\"\"\"\n",
        "\n",
        "        if region.size == 0:\n",
        "            return {'mean_intensity': 0, 'std_intensity': 0, 'contrast': 0}\n",
        "\n",
        "        gray = cv2.cvtColor(region, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "        return {\n",
        "            'mean_intensity': np.mean(gray),\n",
        "            'std_intensity': np.std(gray),\n",
        "            'contrast': np.max(gray) - np.min(gray),\n",
        "            'entropy': self._calculate_entropy(gray)\n",
        "        }\n",
        "\n",
        "    def _calculate_entropy(self, gray_image):\n",
        "        \"\"\"Calculate image entropy (texture complexity)\"\"\"\n",
        "\n",
        "        histogram = cv2.calcHist([gray_image], [0], None, [256], [0, 256])\n",
        "        histogram = histogram.ravel() / histogram.sum()\n",
        "\n",
        "        # Remove zero entries\n",
        "        histogram = histogram[histogram > 0]\n",
        "\n",
        "        entropy = -np.sum(histogram * np.log2(histogram))\n",
        "        return entropy\n",
        "\n",
        "# ============================================\n",
        "# 2. Multi-Image Volumetric Analysis\n",
        "# ============================================\n",
        "\n",
        "def analyze_multiple_images(model_path, image_folder, pixel_spacing=1.0, slice_thickness=5.0):\n",
        "    \"\"\"\n",
        "    Analyze multiple images from a scan sequence\n",
        "\n",
        "    Args:\n",
        "        model_path: Path to YOLO model\n",
        "        image_folder: Folder containing sequential scan images\n",
        "        pixel_spacing: Physical size of pixel in mm\n",
        "        slice_thickness: Distance between slices in mm\n",
        "    \"\"\"\n",
        "\n",
        "    analyzer = BrainTumorVolumetricAnalyzer(model_path, pixel_spacing, slice_thickness)\n",
        "\n",
        "    # Get all images\n",
        "    image_paths = sorted(Path(image_folder).glob('*.jpg'))\n",
        "\n",
        "    all_analyses = []\n",
        "    total_volume = 0\n",
        "\n",
        "    print(f\"Analyzing {len(image_paths)} images...\")\n",
        "\n",
        "    for i, img_path in enumerate(image_paths):\n",
        "        analyses, results, _ = analyzer.analyze_single_image(str(img_path))\n",
        "\n",
        "        for analysis in analyses:\n",
        "            analysis['slice_number'] = i\n",
        "            analysis['image_name'] = img_path.name\n",
        "            all_analyses.append(analysis)\n",
        "            total_volume += analysis['volume_average_mm3']\n",
        "\n",
        "    # Calculate aggregate statistics\n",
        "    if all_analyses:\n",
        "        df = pd.DataFrame(all_analyses)\n",
        "\n",
        "        aggregate_stats = {\n",
        "            'total_detections': len(all_analyses),\n",
        "            'total_volume_mm3': total_volume,\n",
        "            'total_volume_ml': total_volume / 1000,\n",
        "            'avg_confidence': df['confidence'].mean(),\n",
        "            'avg_diameter_mm': df['diameter_avg_mm'].mean(),\n",
        "            'volume_range': [df['volume_average_mm3'].min(), df['volume_average_mm3'].max()],\n",
        "            'tumor_classes': df['class'].value_counts().to_dict()\n",
        "        }\n",
        "\n",
        "        return df, aggregate_stats\n",
        "\n",
        "    return None, None\n",
        "\n",
        "# ============================================\n",
        "# 3. Visualization Functions\n",
        "# ============================================\n",
        "\n",
        "def visualize_volumetric_analysis(analysis, img_rgb, results, output_path='volumetric_analysis.png'):\n",
        "    \"\"\"Create comprehensive volumetric visualization\"\"\"\n",
        "\n",
        "    if not analysis:\n",
        "        print(\"No detections to visualize\")\n",
        "        return\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    gs = fig.add_gridspec(3, 4, hspace=0.4, wspace=0.4)\n",
        "\n",
        "    # 1. Original image with detection\n",
        "    ax1 = fig.add_subplot(gs[0, :2])\n",
        "    pred_img = results[0].plot()\n",
        "    ax1.imshow(cv2.cvtColor(pred_img, cv2.COLOR_BGR2RGB))\n",
        "    ax1.set_title('Detected Brain Tumor', fontsize=16, fontweight='bold')\n",
        "    ax1.axis('off')\n",
        "\n",
        "    # 2. Volumetric measurements bar chart\n",
        "    ax2 = fig.add_subplot(gs[0, 2:])\n",
        "    volumes = [analysis[0]['volume_ellipsoid_mm3'],\n",
        "               analysis[0]['volume_prism_mm3'],\n",
        "               analysis[0]['volume_sphere_mm3'],\n",
        "               analysis[0]['volume_average_mm3']]\n",
        "    methods = ['Ellipsoid', 'Prism', 'Sphere', 'Average']\n",
        "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
        "\n",
        "    bars = ax2.bar(methods, volumes, color=colors, edgecolor='black', linewidth=2)\n",
        "    ax2.set_ylabel('Volume (mmÂ³)', fontsize=12, fontweight='bold')\n",
        "    ax2.set_title('Volume Estimation Methods', fontsize=14, fontweight='bold')\n",
        "    ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.1f}',\n",
        "                ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # 3. 2D Measurements\n",
        "    ax3 = fig.add_subplot(gs[1, :2])\n",
        "    ax3.axis('off')\n",
        "\n",
        "    measurements_text = f\"\"\"\n",
        "    ğŸ“ 2D MEASUREMENTS\n",
        "    {'='*50}\n",
        "    Width:              {analysis[0]['width_mm']:.2f} mm\n",
        "    Height:             {analysis[0]['height_mm']:.2f} mm\n",
        "    Area:               {analysis[0]['area_mm2']:.2f} mmÂ²\n",
        "    Average Diameter:   {analysis[0]['diameter_avg_mm']:.2f} mm\n",
        "\n",
        "    ğŸ“¦ 3D VOLUME ESTIMATES\n",
        "    {'='*50}\n",
        "    Ellipsoid Model:    {analysis[0]['volume_ellipsoid_mm3']:.2f} mmÂ³\n",
        "    Rectangular Model:  {analysis[0]['volume_prism_mm3']:.2f} mmÂ³\n",
        "    Spherical Model:    {analysis[0]['volume_sphere_mm3']:.2f} mmÂ³\n",
        "\n",
        "    â­ Average Volume:   {analysis[0]['volume_average_mm3']:.2f} mmÂ³\n",
        "    ğŸ’§ Volume (mL):      {analysis[0]['volume_ml']:.4f} mL\n",
        "\n",
        "    ğŸ” SHAPE ANALYSIS\n",
        "    {'='*50}\n",
        "    Aspect Ratio:       {analysis[0]['aspect_ratio']:.3f}\n",
        "    Circularity:        {analysis[0]['circularity']:.3f}\n",
        "    Shape Type:         {analysis[0]['shape_regularity']}\n",
        "\n",
        "    ğŸ“ LOCATION\n",
        "    {'='*50}\n",
        "    Brain Quadrant:     {analysis[0]['position']['quadrant']}\n",
        "    Relative X:         {analysis[0]['position']['x_ratio']:.3f}\n",
        "    Relative Y:         {analysis[0]['position']['y_ratio']:.3f}\n",
        "    \"\"\"\n",
        "\n",
        "    ax3.text(0.1, 0.95, measurements_text, transform=ax3.transAxes,\n",
        "            fontsize=11, verticalalignment='top', family='monospace',\n",
        "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "\n",
        "    # 4. Shape characteristics radar chart\n",
        "    ax4 = fig.add_subplot(gs[1, 2:], projection='polar')\n",
        "\n",
        "    categories = ['Confidence', 'Circularity', 'Aspect\\nRatio\\n(norm)', 'Texture\\nContrast\\n(norm)']\n",
        "    values = [\n",
        "        analysis[0]['confidence'],\n",
        "        analysis[0]['circularity'],\n",
        "        min(analysis[0]['aspect_ratio'], 2.0) / 2.0,  # Normalize\n",
        "        min(analysis[0]['texture']['contrast'] / 255.0, 1.0)\n",
        "    ]\n",
        "\n",
        "    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
        "    values += values[:1]\n",
        "    angles += angles[:1]\n",
        "\n",
        "    ax4.plot(angles, values, 'o-', linewidth=2, color='#FF6B6B')\n",
        "    ax4.fill(angles, values, alpha=0.25, color='#FF6B6B')\n",
        "    ax4.set_xticks(angles[:-1])\n",
        "    ax4.set_xticklabels(categories)\n",
        "    ax4.set_ylim(0, 1)\n",
        "    ax4.set_title('Tumor Characteristics', fontsize=14, fontweight='bold', pad=20)\n",
        "    ax4.grid(True)\n",
        "\n",
        "    # 5. 3D Volume visualization (simplified representation)\n",
        "    ax5 = fig.add_subplot(gs[2, :2], projection='3d')\n",
        "\n",
        "    # Create ellipsoid representation\n",
        "    a = analysis[0]['width_mm'] / 2\n",
        "    b = analysis[0]['height_mm'] / 2\n",
        "    c = (a + b) / 2\n",
        "\n",
        "    u = np.linspace(0, 2 * np.pi, 50)\n",
        "    v = np.linspace(0, np.pi, 50)\n",
        "    x = a * np.outer(np.cos(u), np.sin(v))\n",
        "    y = b * np.outer(np.sin(u), np.sin(v))\n",
        "    z = c * np.outer(np.ones(np.size(u)), np.cos(v))\n",
        "\n",
        "    ax5.plot_surface(x, y, z, alpha=0.7, cmap='Reds', edgecolor='none')\n",
        "    ax5.set_xlabel('Width (mm)', fontweight='bold')\n",
        "    ax5.set_ylabel('Height (mm)', fontweight='bold')\n",
        "    ax5.set_zlabel('Depth (mm)', fontweight='bold')\n",
        "    ax5.set_title('3D Tumor Model (Ellipsoid)', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # 6. Texture analysis\n",
        "    ax6 = fig.add_subplot(gs[2, 2:])\n",
        "    texture_metrics = ['Mean\\nIntensity', 'Std Dev', 'Contrast', 'Entropy']\n",
        "    texture_values = [\n",
        "        analysis[0]['texture']['mean_intensity'] / 255,\n",
        "        analysis[0]['texture']['std_intensity'] / 128,\n",
        "        analysis[0]['texture']['contrast'] / 255,\n",
        "        analysis[0]['texture']['entropy'] / 8\n",
        "    ]\n",
        "\n",
        "    colors_texture = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
        "    bars = ax6.barh(texture_metrics, texture_values, color=colors_texture, edgecolor='black', linewidth=2)\n",
        "    ax6.set_xlabel('Normalized Value', fontsize=12, fontweight='bold')\n",
        "    ax6.set_title('Texture Characteristics', fontsize=14, fontweight='bold')\n",
        "    ax6.set_xlim(0, 1)\n",
        "    ax6.grid(axis='x', alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for i, (bar, val) in enumerate(zip(bars, texture_values)):\n",
        "        ax6.text(val + 0.02, i, f'{val:.3f}', va='center', fontweight='bold')\n",
        "\n",
        "    # Main title\n",
        "    tumor_class = analysis[0]['class']\n",
        "    confidence = analysis[0]['confidence'] * 100\n",
        "    plt.suptitle(f'Brain Tumor Volumetric Analysis\\n{tumor_class.upper()} - {confidence:.1f}% Confidence',\n",
        "                fontsize=18, fontweight='bold', y=0.98)\n",
        "\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
        "    print(f\"âœ… Volumetric analysis saved to: {output_path}\")\n",
        "    plt.show()\n",
        "\n",
        "# ============================================\n",
        "# 4. Generate Detailed Report\n",
        "# ============================================\n",
        "\n",
        "def generate_volumetric_report(analysis):\n",
        "    \"\"\"Generate detailed volumetric analysis report\"\"\"\n",
        "\n",
        "    if not analysis:\n",
        "        print(\"No detections to report\")\n",
        "        return\n",
        "\n",
        "    a = analysis[0]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ğŸ§  BRAIN TUMOR VOLUMETRIC ANALYSIS REPORT\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"\\nğŸ“‹ TUMOR CLASSIFICATION\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"  Type:           {a['class'].upper()}\")\n",
        "    print(f\"  Confidence:     {a['confidence']*100:.2f}%\")\n",
        "    print(f\"  Detection Box:  ({a['bbox'][0]}, {a['bbox'][1]}) â†’ ({a['bbox'][2]}, {a['bbox'][3]})\")\n",
        "\n",
        "    print(f\"\\nğŸ“ 2D MEASUREMENTS\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"  Width:                {a['width_mm']:.2f} mm\")\n",
        "    print(f\"  Height:               {a['height_mm']:.2f} mm\")\n",
        "    print(f\"  Cross-sectional Area: {a['area_mm2']:.2f} mmÂ²\")\n",
        "    print(f\"  Average Diameter:     {a['diameter_avg_mm']:.2f} mm\")\n",
        "\n",
        "    print(f\"\\nğŸ“¦ 3D VOLUME ESTIMATES\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"  Ellipsoid Model:      {a['volume_ellipsoid_mm3']:.2f} mmÂ³\")\n",
        "    print(f\"  Rectangular Model:    {a['volume_prism_mm3']:.2f} mmÂ³\")\n",
        "    print(f\"  Spherical Model:      {a['volume_sphere_mm3']:.2f} mmÂ³\")\n",
        "    print(f\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
        "    print(f\"  Average Estimate:     {a['volume_average_mm3']:.2f} mmÂ³\")\n",
        "    print(f\"  Volume in mL:         {a['volume_ml']:.4f} mL\")\n",
        "    print(f\"  Volume in cmÂ³:        {a['volume_average_mm3']/1000:.4f} cmÂ³\")\n",
        "\n",
        "    print(f\"\\nğŸ” SHAPE ANALYSIS\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"  Aspect Ratio:         {a['aspect_ratio']:.3f}\")\n",
        "    print(f\"  Circularity Index:    {a['circularity']:.3f} (1.0 = perfect circle)\")\n",
        "    print(f\"  Shape Classification: {a['shape_regularity']}\")\n",
        "\n",
        "    print(f\"\\nğŸ“ ANATOMICAL LOCATION\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"  Brain Quadrant:       {a['position']['quadrant']}\")\n",
        "    print(f\"  Horizontal Position:  {a['position']['x_ratio']*100:.1f}% from left\")\n",
        "    print(f\"  Vertical Position:    {a['position']['y_ratio']*100:.1f}% from top\")\n",
        "\n",
        "    print(f\"\\nğŸ¨ TEXTURE CHARACTERISTICS\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"  Mean Intensity:       {a['texture']['mean_intensity']:.2f}\")\n",
        "    print(f\"  Intensity Std Dev:    {a['texture']['std_intensity']:.2f}\")\n",
        "    print(f\"  Contrast:             {a['texture']['contrast']:.2f}\")\n",
        "    print(f\"  Entropy:              {a['texture']['entropy']:.2f}\")\n",
        "\n",
        "    print(f\"\\nğŸ’¡ CLINICAL NOTES\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # Size classification\n",
        "    if a['diameter_avg_mm'] < 10:\n",
        "        size_class = \"Small (<10mm)\"\n",
        "        note = \"May require monitoring\"\n",
        "    elif a['diameter_avg_mm'] < 30:\n",
        "        size_class = \"Medium (10-30mm)\"\n",
        "        note = \"Clinical evaluation recommended\"\n",
        "    else:\n",
        "        size_class = \"Large (>30mm)\"\n",
        "        note = \"Immediate clinical attention advised\"\n",
        "\n",
        "    print(f\"  Size Classification:  {size_class}\")\n",
        "    print(f\"  Note:                 {note}\")\n",
        "\n",
        "    # Shape analysis\n",
        "    if a['circularity'] > 0.8:\n",
        "        shape_note = \"Regular round shape - typical for benign tumors\"\n",
        "    elif a['circularity'] < 0.5:\n",
        "        shape_note = \"Irregular shape - may warrant further investigation\"\n",
        "    else:\n",
        "        shape_note = \"Moderate irregularity - standard clinical protocols apply\"\n",
        "\n",
        "    print(f\"  Shape Assessment:     {shape_note}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"âš ï¸  DISCLAIMER: This analysis is for research purposes only.\")\n",
        "    print(\"    All clinical decisions should be made by qualified medical professionals.\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# ============================================\n",
        "# 5. Main Execution\n",
        "# ============================================\n",
        "\n",
        "# Configuration\n",
        "MODEL_PATH = \"/content/runs/detect/train/weights/best.pt\"\n",
        "IMAGE_PATH = \"/content/Brain-Tumor-Detection-1/test/images/Te-me_0159_jpg.rf.7afad29587df72dcfba2e189a75656fa.jpg\"\n",
        "\n",
        "# Set physical parameters (adjust based on your MRI/CT scan specifications)\n",
        "PIXEL_SPACING = 0.5  # mm per pixel (typical for brain MRI: 0.5-1.0 mm)\n",
        "SLICE_THICKNESS = 5.0  # mm between slices (typical: 1-5 mm)\n",
        "\n",
        "print(\"ğŸ”¬ Initializing Brain Tumor Volumetric Analyzer...\")\n",
        "print(f\"   Pixel Spacing: {PIXEL_SPACING} mm\")\n",
        "print(f\"   Slice Thickness: {SLICE_THICKNESS} mm\\n\")\n",
        "\n",
        "# Create analyzer\n",
        "analyzer = BrainTumorVolumetricAnalyzer(MODEL_PATH, PIXEL_SPACING, SLICE_THICKNESS)\n",
        "\n",
        "# Analyze image\n",
        "print(\"ğŸ” Analyzing brain tumor...\")\n",
        "analyses, results, img_rgb = analyzer.analyze_single_image(IMAGE_PATH)\n",
        "\n",
        "if analyses:\n",
        "    print(f\"âœ… Detected {len(analyses)} tumor(s)\\n\")\n",
        "\n",
        "    # Generate visualization\n",
        "    print(\"ğŸ“Š Creating volumetric visualization...\")\n",
        "    visualize_volumetric_analysis(analyses, img_rgb, results, 'brain_tumor_volumetric.png')\n",
        "\n",
        "    # Display result\n",
        "    from IPython.display import Image, display\n",
        "    display(Image('brain_tumor_volumetric.png'))\n",
        "\n",
        "    # Generate detailed report\n",
        "    generate_volumetric_report(analyses)\n",
        "\n",
        "    # Save results to JSON\n",
        "    with open('volumetric_analysis.json', 'w') as f:\n",
        "        json.dump(analyses, f, indent=2)\n",
        "    print(\"ğŸ’¾ Analysis saved to: volumetric_analysis.json\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ No tumors detected in the image\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyP/4spNHjHQuIWoHmRhg2+V",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}